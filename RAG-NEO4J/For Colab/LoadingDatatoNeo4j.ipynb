{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4URbTipP4mS"
      },
      "outputs": [],
      "source": [
        "pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain neo4j python-dotenv nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZMnhcsxat58"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain-together #may cause dependency issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J2gJuobhE8l"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv8XnmMfiSSH",
        "outputId": "27083daa-a716-4c49-8a22-4493e4b83cb6"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet langchain_experimental langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load All Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG6EDe2wO_y1"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "from langchain.chains.openai_functions import create_structured_output_chain\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain_community.graphs import Neo4jGraph\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_text_splitters import TokenTextSplitter\n",
        "from neo4j.exceptions import ClientError\n",
        "from neo4j import GraphDatabase\n",
        "\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_text_splitters import TokenTextSplitter\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "#\n",
        "\n",
        "import openai\n",
        "\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "\n",
        "import together\n",
        "from openai import OpenAI\n",
        "from langchain_together import Together\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "\n",
        "# Access the variables\n",
        "#openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "#neo4j_uri = os.getenv(\"NEO4J_URI\")\n",
        "#neo4j_username = os.getenv(\"NEO4J_USERNAME\")\n",
        "#neo4j_password = os.getenv(\"NEO4J_PASSWORD\")\n",
        "##neo4j_db = os.getenv(\"NEO4J_DB\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6H4V9nsZlq2"
      },
      "source": [
        "ALL KEYS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BwZUNfmZN3X"
      },
      "outputs": [],
      "source": [
        "openai_api_key = # <your_openai_api_key>\n",
        "uri = # <your_neo4j_uri>\n",
        "username = # <your_neo4j_username>\n",
        "password = # <your_neo4j_password>\n",
        "\n",
        "HF_token = # <your_huggingface_token>\n",
        "\n",
        "togetherai_api_key = # <your_togetherai_api_key>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgOVIYsDPWdX",
        "outputId": "a11931c9-982e-4f7d-bd96-f6d64db70fc0"
      },
      "outputs": [],
      "source": [
        "##To verify your connection with neo4j instance\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "try:\n",
        "    with driver.session() as session:\n",
        "        result = session.run(\"RETURN 'Hello, World!' AS greeting\")\n",
        "        print(result.single()[\"greeting\"])\n",
        "finally:\n",
        "    driver.close()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOEpzxQnZjQy"
      },
      "source": [
        "CHUNKERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtNVR6hzZbol"
      },
      "outputs": [],
      "source": [
        "#pip install nltk\n",
        "\n",
        "\n",
        "def recursive(txt_doc):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = 1000,\n",
        "        chunk_overlap  = 100,\n",
        "        length_function = len,\n",
        "        is_separator_regex = False,\n",
        "    )\n",
        "    # text splitter\n",
        "    splits = text_splitter.split_documents(txt_doc)\n",
        "    return splits\n",
        "\n",
        "\n",
        "def character(txt_doc):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "    separator = \".\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap = 80 #always less than chunk size\n",
        "    )\n",
        "    characters = text_splitter.split_text(txt_doc)\n",
        "    return characters\n",
        "\n",
        "\n",
        "def sentence(txt_doc):\n",
        "    nltk.download('punkt')\n",
        "    sentences = sent_tokenize(txt_doc)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def paragraphs(text):\n",
        "    paragraphs = text.split('\\n\\n')  # Assuming paragraphs are separated by two newlines\n",
        "    return paragraphs\n",
        "\n",
        "def semantic(text):\n",
        "    if not isinstance(text, str):\n",
        "        raise TypeError(\"The 'text' argument must be a string.\")\n",
        "    semantic_splits = SemanticChunker(OpenAIEmbeddings(openai_api_key=openai_api_key))\n",
        "    docs = semantic_splits.create_documents([text])\n",
        "    return docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgYVPojaZqLo"
      },
      "source": [
        "EMBEDDINGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqVd1gGcZr-x"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lc_openai_embedding():\n",
        "     model=\"text-embedding-3-large\" ## 3072 dimension embeddings\n",
        "     embeddings = OpenAIEmbeddings(\n",
        "            model = model,\n",
        "            dimensions = 1536,\n",
        "            openai_api_key=openai_api_key)\n",
        "     return embeddings\n",
        "\n",
        "def openai_embedding(): #1536 dim\n",
        "    model=\"text-embedding-ada-002\"\n",
        "    embeddings = OpenAIEmbeddings(\n",
        "        model = model,\n",
        "        openai_api_key=openai_api_key)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def generate_huggingface_embeddings(): #1024 dim\n",
        "    model_name = \"BAAI/bge-large-en-v1.5\"  #BAAI/bge-large-en-v1.5, BAAI/bge-large-en, sangmini/msmarco-cotmae-MiniLM-L12_en-ko-ja  #https://huggingface.co/BAAI/bge-small-en-v1.5\n",
        "    model_kwargs = {'device': 'cpu'}\n",
        "    encode_kwargs = {'normalize_embeddings': False}\n",
        "    embeddings = HuggingFaceBgeEmbeddings(\n",
        "        model_name=model_name,\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "        )\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "def generate_gpt4all(): #384 dim\n",
        "   embeddings = GPT4AllEmbeddings(openai_api_type=openai_api_key) \n",
        "   return embeddings\n",
        "\n",
        "#with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
        "#    content = file.read()\n",
        "#   print(content)\n",
        "\n",
        "#get_openai_embedding(content)\n",
        "\n",
        "#generate_gpt4all(content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMdwVUmgaAR7"
      },
      "source": [
        "Large Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evbZ7fmZaFny"
      },
      "outputs": [],
      "source": [
        "#%pip install --upgrade --quiet langchain-together\n",
        "\n",
        "client = OpenAI(api_key = openai_api_key)\n",
        "\n",
        "def infer_Mixtral(prompt):\n",
        "    llm = Together(\n",
        "    model=\"mistralai/Mixtral-8x22B\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024,\n",
        "    together_api_key=togetherai_api_key\n",
        "    )\n",
        "    candidate = llm.invoke(prompt)\n",
        "    return candidate\n",
        "\n",
        "\n",
        "def infer_llama3(prompt):\n",
        "    llm = Together(\n",
        "    model=\"meta-llama/Llama-3-8b-chat-hf\", #\"meta-llama/Llama-3-8b-hf\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024,\n",
        "    together_api_key=togetherai_api_key\n",
        "    )\n",
        "    candidate = llm.invoke(prompt)\n",
        "    return candidate\n",
        "\n",
        "def infer_llama2(prompt):\n",
        "    llm = Together(\n",
        "    model=\"togethercomputer/LLaMA-2-7B-32K\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024,\n",
        "    together_api_key=togetherai_api_key\n",
        "    )\n",
        "    candidate = llm.invoke(prompt)\n",
        "    return candidate\n",
        "\n",
        "def infer_Qwen(prompt):\n",
        "    llm = Together(\n",
        "    model=\"Qwen/Qwen1.5-1.8B-Chat\",\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024,\n",
        "    together_api_key=togetherai_api_key\n",
        "    )\n",
        "    candidate = llm.invoke(prompt)\n",
        "    return candidate\n",
        "\n",
        "\n",
        "def infer_gpt4(prompt):\n",
        "    llm = ChatOpenAI(openai_api_type=openai_api_key)\n",
        "    prompt=prompt\n",
        "    candidate = llm.invoke(prompt,model='gpt-4-turbo-preview')\n",
        "    return candidate.content\n",
        "\n",
        "\n",
        "# # Example usage\n",
        "# prompt_text = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
        "# response_text = infer_llama2(prompt_text)\n",
        "# print(response_text)\n",
        "\n",
        "#repo_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "repo_id = \"Qwen/Qwen1.5-7B-Chat-GPTQ-Int8\"\n",
        "#repo_id = \"google/gemma-7b\"\n",
        "\n",
        "def infer_HF(prompt):\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=repo_id, max_length=128, temperature=0.5, token=HF_token\n",
        "        )\n",
        "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "    return llm_chain.run(prompt)\n",
        "\n",
        "\n",
        "# prompt_text = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
        "# response_text = infer_HF(prompt_text)\n",
        "# print(response_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJEtjKOKbLH6"
      },
      "source": [
        "PDF LOADING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aWrzN8APWj5"
      },
      "outputs": [],
      "source": [
<<<<<<< HEAD:RAG-NEO4J/LoadingDatatoNeo4j.ipynb
        "pdfpath=\"..\\source\\Constitution.pdf\"\n",
=======
        "pdfpath=\"input/Constitution.pdf\"\n",
>>>>>>> origin/main:RAG-NEO4J/For Colab/LoadingDatatoNeo4j.ipynb
        "loader = PyPDFLoader(pdfpath)\n",
        "documents = loader.load()\n",
        "# Assume each document loaded is a full page and corresponds to a parent node\n",
        "parent_documents = documents  # No need for parent_splitter if each document is a full page\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79o_qigylC3O",
        "outputId": "750393bd-08d1-4c72-e993-b61503213e23"
      },
      "outputs": [],
      "source": [
        "parent_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InPrcx9beEcw"
      },
      "source": [
        "CREATING NEO4J GRAPH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuaTTrKzPWmW"
      },
      "outputs": [],
      "source": [
        "graph = Neo4jGraph(\n",
        "    url=uri, username=username, password=password\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W255ET1_L5ph"
      },
      "outputs": [],
      "source": [
        "#Select Options\n",
        "embeddingtype = 'langchain'\n",
        "llmtype = 'Qwen' #llama2, llama3, Qwen, empty string will invoke Mixtral\n",
        "embedding_dimension = 1536  ##change to 384 if using gpt4all embedding\n",
        "\n",
        "if embeddingtype == 'openai':\n",
        "    embeddings = openai_embedding()\n",
        "elif embeddingtype == 'HF':\n",
        "    embeddings = generate_huggingface_embeddings()\n",
        "elif embeddingtype == 'langchain':\n",
        "    embeddings = lc_openai_embedding()\n",
        "else:\n",
        "    embeddings = generate_gpt4all()\n",
        "\n",
        "\n",
        "if llmtype == 'gpt4':\n",
        "    llm = ChatOpenAI(temperature=0,\n",
        "                     openai_api_key=openai_api_key,\n",
        "                     model='gpt-4-turbo-preview')\n",
        "elif llmtype == 'llama2':\n",
        "      llm = Together(\n",
        "          model=\"togethercomputer/LLaMA-2-7B-32K\",\n",
        "          temperature=0.5,\n",
        "          max_tokens=1024,\n",
        "          together_api_key=togetherai_api_key)\n",
        "elif llmtype == 'llama3':\n",
        "      llm = Together(\n",
        "          model=\"meta-llama/Llama-3-8b-chat-hf\",\n",
        "          temperature=0.5,\n",
        "          max_tokens=1024,\n",
        "          together_api_key=togetherai_api_key)\n",
        "elif llmtype == 'Qwen':\n",
        "      llm = Together(\n",
        "          model=\"Qwen/Qwen1.5-1.8B-Chat\",\n",
        "          temperature=0.5,\n",
        "          max_tokens=1024,\n",
        "          together_api_key=togetherai_api_key)\n",
        "else:\n",
        "      llm = Together(\n",
        "          model=\"mistralai/Mixtral-8x22B\",\n",
        "          temperature=0.5,\n",
        "          max_tokens=1024,\n",
        "          together_api_key=togetherai_api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RJRiltvSkm4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHwVfBhVPWo-"
      },
      "outputs": [],
      "source": [
        "# You can use the functions defined above or use the following\n",
        "\n",
        "#embedding funtions: get_openai_embedding, generate_huggingface_embeddings, generate_gpt4all\n",
        "#splitters:recursive, character, sentence, paragraphs, semantic\n",
        "\n",
        "# Ingest Parent-Child node pairs\n",
        "\n",
        "for i, parent in enumerate(parent_documents):\n",
        "    # Create child splits from each parent page if necessary\n",
        "    child_documents = semantic(str(parent))  ##Splitters are called here\n",
        "    page_metadata = parent.metadata  # Access metadata, adjust according to your data structure\n",
        "    params = {\n",
        "        \"parent_text\": parent.page_content,\n",
        "        \"parent_id\": f\"{page_metadata['source']}-{page_metadata['page']}\",  # Unique ID using source and page number\n",
        "        \"parent_embedding\": embeddings.embed_query(parent.page_content),\n",
        "        \"children\": [\n",
        "            {\n",
        "                \"text\": c.page_content,\n",
        "                \"id\": f\"{page_metadata['source']}-{page_metadata['page']}-{ic}\",\n",
        "                \"embedding\": embeddings.embed_query(c.page_content),\n",
        "            }\n",
        "            for ic, c in enumerate(child_documents)\n",
        "        ],\n",
        "    }\n",
        "    # Ingest data with updated query using metadata in IDs\n",
        "    graph.query(\n",
        "        \"\"\"\n",
        "        MERGE (p:Parent {id: $parent_id})\n",
        "        SET p.text = $parent_text\n",
        "        WITH p\n",
        "        CALL db.create.setVectorProperty(p, 'embedding', $parent_embedding)\n",
        "        YIELD node\n",
        "        WITH p\n",
        "        UNWIND $children AS child\n",
        "        MERGE (c:Child {id: child.id})\n",
        "        SET c.text = child.text\n",
        "        MERGE (c)<-[:HAS_CHILD]-(p)\n",
        "        WITH c, child\n",
        "        CALL db.create.setVectorProperty(c, 'embedding', child.embedding)\n",
        "        YIELD node\n",
        "        RETURN count(*)\n",
        "        \"\"\",\n",
        "        params,\n",
        "    )\n",
        "    # Create vector index for child\n",
        "    try:\n",
        "        graph.query(\n",
        "            \"CALL db.index.vector.createNodeIndex('parent_document', \"\n",
        "            \"'Child', 'embedding', $dimension, 'cosine')\",\n",
        "            {\"dimension\": embedding_dimension},\n",
        "        )\n",
        "    except ClientError:  # already exists\n",
        "        pass\n",
        "    # Create vector index for parents\n",
        "    try:\n",
        "        graph.query(\n",
        "            \"CALL db.index.vector.createNodeIndex('typical_rag', \"\n",
        "            \"'Parent', 'embedding', $dimension, 'cosine')\",\n",
        "            {\"dimension\": embedding_dimension},\n",
        "        )\n",
        "    except ClientError:  # already exists\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-E-3_IeeLXM"
      },
      "source": [
        "Generating Questions from the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1D4c4SGPWr2"
      },
      "outputs": [],
      "source": [
        "# Ingest hypothethical questions\n",
        "\n",
        "\n",
        "class Questions(BaseModel):\n",
        "    \"\"\"Generating hypothetical questions about text.\"\"\"\n",
        "\n",
        "    questions: List[str] = Field(\n",
        "        ...,\n",
        "        description=(\n",
        "            \"Generated hypothetical questions based on \" \"the information from the text\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "questions_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            (\n",
        "                \"You are generating hypothetical questions based on the information \"\n",
        "                \"found in the text. Make sure to provide full context in the generated \"\n",
        "                \"questions.\"\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            (\n",
        "                \"Use the given format to generate hypothetical questions from the \"\n",
        "                \"following input: {input}\"\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "question_chain = create_structured_output_chain(Questions, llm, questions_prompt)\n",
        "\n",
        "for i, parent in enumerate(parent_documents):\n",
        "    questions = question_chain.run(parent.page_content).questions\n",
        "    params = {\n",
        "        \"parent_id\": i,\n",
        "        \"questions\": [\n",
        "            {\"text\": q, \"id\": f\"{i}-{iq}\", \"embedding\": embeddings.embed_query(q)}\n",
        "            for iq, q in enumerate(questions)\n",
        "            if q\n",
        "        ],\n",
        "    }\n",
        "    # Print the generated questions\n",
        "    print(f\"Generated questions for Parent ID {i}:\")\n",
        "    for question in questions:\n",
        "        print(question)\n",
        "\n",
        "\n",
        "    graph.query(\n",
        "        \"\"\"\n",
        "    MERGE (p:Parent {id: $parent_id})\n",
        "    WITH p\n",
        "    UNWIND $questions AS question\n",
        "    CREATE (q:Question {id: question.id})\n",
        "    SET q.text = question.text\n",
        "    MERGE (q)<-[:HAS_QUESTION]-(p)\n",
        "    WITH q, question\n",
        "    CALL db.create.setVectorProperty(q, 'embedding', question.embedding)\n",
        "    YIELD node\n",
        "    RETURN count(*)\n",
        "    \"\"\",\n",
        "        params,\n",
        "    )\n",
        "    # Create vector index\n",
        "    try:\n",
        "        graph.query(\n",
        "            \"CALL db.index.vector.createNodeIndex('hypothetical_questions', \"\n",
        "            \"'Question', 'embedding', $dimension, 'cosine')\",\n",
        "            {\"dimension\": embedding_dimension},\n",
        "        )\n",
        "    except ClientError:  # already exists\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjJrAHzZPWyP"
      },
      "outputs": [],
      "source": [
        "# Ingest summaries\n",
        "\n",
        "summary_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            (\n",
        "                \"You are generating concise and accurate summaries based on the \"\n",
        "                \"information found in the text.\"\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            (\"Generate a summary of the following input: {question}\\n\" \"Summary:\"),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "summary_chain = summary_prompt | llm\n",
        "\n",
        "for i, parent in enumerate(parent_documents):\n",
        "    summary = summary_chain.invoke({\"question\": parent.page_content}).content\n",
        "    params = {\n",
        "        \"parent_id\": i,\n",
        "        \"summary\": summary,\n",
        "        \"embedding\": embeddings.embed_query(summary),\n",
        "    }\n",
        "    print(f\"Generated summary for Parent ID {i}:\")\n",
        "    print(summary)\n",
        "\n",
        "    graph.query(\n",
        "        \"\"\"\n",
        "    MERGE (p:Parent {id: $parent_id})\n",
        "    MERGE (p)-[:HAS_SUMMARY]->(s:Summary)\n",
        "    SET s.text = $summary\n",
        "    WITH s\n",
        "    CALL db.create.setVectorProperty(s, 'embedding', $embedding)\n",
        "    YIELD node\n",
        "    RETURN count(*)\n",
        "    \"\"\",\n",
        "        params,\n",
        "    )\n",
        "    # Create vector index\n",
        "    try:\n",
        "        graph.query(\n",
        "            \"CALL db.index.vector.createNodeIndex('summary', \"\n",
        "            \"'Summary', 'embedding', $dimension, 'cosine')\",\n",
        "            {\"dimension\": embedding_dimension},\n",
        "        )\n",
        "    except ClientError:  # already exists\n",
        "        pass"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
